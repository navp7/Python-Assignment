{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c71c5b2-7fcd-4ca4-b81f-c1ca71a60a07",
   "metadata": {},
   "source": [
    "## Logistic Regression-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e27f7-69e4-4694-97d4-4b1eb18467a4",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db3ebb-2207-4eeb-913d-1d647296a16b",
   "metadata": {},
   "source": [
    "**Linear Regression** and **Logistic Regression** are both types of regression analysis, but they serve different purposes and are suitable for different types of problems:\n",
    "\n",
    "**Linear Regression**:\n",
    "\n",
    "1. **Purpose**: Linear regression is used to model and analyze the relationship between a continuous dependent variable (target) and one or more independent variables (features). It predicts a continuous numeric value.\n",
    "\n",
    "2. **Output**: The output is a continuous value, which can be any real number.\n",
    "\n",
    "3. **Use Cases**: It is suitable for regression problems, such as predicting house prices, stock prices, or any problem where you want to estimate a numeric value.\n",
    "\n",
    "4. **Example**: Predicting the price of a house based on its square footage, number of bedrooms, and other features. The output is a price, which is a continuous variable.\n",
    "\n",
    "**Logistic Regression**:\n",
    "\n",
    "1. **Purpose**: Logistic regression is used to model and analyze the relationship between a binary dependent variable (target) and one or more independent variables (features). It predicts the probability of an observation belonging to a particular category.\n",
    "\n",
    "2. **Output**: The output is a probability score between 0 and 1. It is often used to classify observations into two classes, typically represented as 0 (negative class) and 1 (positive class).\n",
    "\n",
    "3. **Use Cases**: It is suitable for classification problems, such as spam detection (classifying emails as spam or not spam), disease prediction (classifying patients as having a disease or not), and sentiment analysis (classifying text as positive or negative sentiment).\n",
    "\n",
    "4. **Example**: Determining whether an email is spam or not based on the presence of certain keywords and patterns in the email's content. The output is a probability score that the email is spam (1) or not spam (0).\n",
    "\n",
    "**Scenario Where Logistic Regression is More Appropriate**:\n",
    "\n",
    "Consider a scenario where you want to predict whether a student will pass (1) or fail (0) an exam based on factors like the number of hours studied and previous exam scores. Logistic regression would be more appropriate in this case because it deals with binary classification problems where the outcome is categorical, such as pass/fail, yes/no, or spam/not spam.\n",
    "\n",
    "In this scenario, you want to determine the probability of a student passing the exam based on their features. Logistic regression can provide a probability score between 0 and 1, making it suitable for this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0389fb-5d6e-4caf-bb8c-d7f38a98b10a",
   "metadata": {},
   "source": [
    "### Q2. What is the cost function used in logistic regression, and how is it optimized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42212452-b7b9-4afa-874b-f5eceff18cdc",
   "metadata": {},
   "source": [
    "In logistic regression, the cost function used is the **logistic loss function**, also known as the **log loss** or **cross-entropy loss**. The logistic loss function is used to measure how well a logistic regression model predicts the probability of an observation belonging to a particular class (e.g., class 1) compared to the true class labels (0 or 1).\n",
    "\n",
    "The logistic loss function for a binary classification problem is defined as:\n",
    "\n",
    "\\[ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] \\]\n",
    "\n",
    "Where:\n",
    "- \\(J(\\theta)\\) is the cost function that we want to minimize.\n",
    "- \\(m\\) is the number of training examples.\n",
    "- \\(y^{(i)}\\) is the true class label for the \\(i\\)-th training example (0 or 1).\n",
    "- \\(h_\\theta(x^{(i)})\\) is the predicted probability that the \\(i\\)-th example belongs to class 1, as predicted by the logistic regression model.\n",
    "\n",
    "The goal in logistic regression is to find the values of the model parameters (\\(\\theta\\)) that minimize this cost function. This is typically done using optimization techniques, such as gradient descent.\n",
    "\n",
    "**Optimizing the Cost Function**:\n",
    "\n",
    "Gradient descent is commonly used to optimize the logistic loss function and find the optimal values of the model parameters (\\(\\theta\\)). The process involves iteratively updating the parameter values to minimize the cost function. Here are the steps in gradient descent:\n",
    "\n",
    "1. Initialize the model parameters (\\(\\theta\\)) with some initial values.\n",
    "\n",
    "2. Calculate the gradient of the cost function with respect to the parameters (\\(\\nabla J(\\theta)\\)).\n",
    "\n",
    "3. Update the parameters using the gradient and a learning rate (\\(\\alpha\\)):\n",
    "\n",
    "   \\[ \\theta := \\theta - \\alpha \\nabla J(\\theta) \\]\n",
    "\n",
    "4. Repeat steps 2 and 3 until convergence or for a fixed number of iterations.\n",
    "\n",
    "The learning rate (\\(\\alpha\\)) controls the step size in the parameter space during each update. Careful selection of the learning rate is essential to ensure efficient convergence. Too large a learning rate can cause divergence, while too small a learning rate can slow down convergence.\n",
    "\n",
    "Optimizing the cost function via gradient descent continues until the model parameters reach values that minimize the logistic loss, resulting in a well-fitted logistic regression model that accurately predicts class probabilities for binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3dc294-ecff-47d5-8fd1-dbe6a1111762",
   "metadata": {},
   "source": [
    "### Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ae8dd-173f-43a4-aa82-66590d8c307d",
   "metadata": {},
   "source": [
    "Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the logistic loss function. It encourages the model to generalize better to unseen data by discouraging overly complex models with overly large coefficients. There are two common types of regularization used in logistic regression: **L1 regularization** (Lasso) and **L2 regularization** (Ridge).\n",
    "\n",
    "Here's how regularization works in logistic regression and its role in preventing overfitting:\n",
    "\n",
    "1. **Regularized Logistic Loss Function**:\n",
    "\n",
    "   In the logistic loss function with L1 (Lasso) regularization, the cost function is modified as follows:\n",
    "\n",
    "   \\[ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] + \\lambda \\sum_{j=1}^{n} |\\theta_j| \\]\n",
    "\n",
    "   In L2 (Ridge) regularization, it is modified as:\n",
    "\n",
    "   \\[ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\]\n",
    "\n",
    "   In both cases, the regularization term is represented by the sum of the absolute values of the model coefficients (\\(L1\\)) or the sum of the squares of the coefficients (\\(L2\\)), multiplied by a regularization parameter (\\(\\lambda\\)). The higher the value of \\(\\lambda\\), the stronger the regularization effect.\n",
    "\n",
    "2. **Regularization Strength**:\n",
    "\n",
    "   The regularization parameter (\\(\\lambda\\)) controls the trade-off between fitting the training data well (minimizing the logistic loss) and preventing overfitting by reducing the magnitude of the coefficients. A larger \\(\\lambda\\) encourages smaller coefficients, which can lead to a simpler model with reduced variance but potentially increased bias.\n",
    "\n",
    "3. **Effect on Model Coefficients**:\n",
    "\n",
    "   Regularization has the effect of shrinking the model coefficients (parameters). The larger the \\(\\lambda\\), the more the coefficients are shrunk towards zero. This helps in reducing the complexity of the model and prevents it from fitting the noise in the training data.\n",
    "\n",
    "4. **Preventing Overfitting**:\n",
    "\n",
    "   By adding the regularization term to the cost function, logistic regression is encouraged to find a balance between fitting the training data well and preventing the model from becoming overly complex. The regularization term discourages the coefficients from becoming too large, making the model less prone to overfitting.\n",
    "\n",
    "In summary, regularization in logistic regression helps prevent overfitting by introducing a penalty term in the cost function that discourages overly large coefficients. This results in a more generalizable model that performs better on unseen data. The choice of the regularization type (\\(L1\\) or \\(L2\\)) and the regularization strength (\\(\\lambda\\)) should be carefully tuned to achieve the desired balance between bias and variance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cdb975-fa41-46b1-a306-73167c11d494",
   "metadata": {},
   "source": [
    "### Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfc357-34d3-434e-9cad-dbc7626bf5f4",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical tool used to evaluate the performance of binary classification models, including logistic regression models. It visually summarizes the trade-off between the true positive rate (sensitivity) and the false positive rate as you adjust the model's classification threshold.\n",
    "\n",
    "Here's how the ROC curve is constructed and used to evaluate the performance of a logistic regression model:\n",
    "\n",
    "1. **True Positive Rate (Sensitivity)**: The true positive rate, often called sensitivity or recall, measures the proportion of actual positive instances (e.g., the presence of a disease) that the model correctly classifies as positive.\n",
    "\n",
    "   True Positive Rate = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "2. **False Positive Rate (1 - Specificity)**: The false positive rate measures the proportion of actual negative instances that the model incorrectly classifies as positive.\n",
    "\n",
    "   False Positive Rate = False Positives / (False Positives + True Negatives)\n",
    "\n",
    "3. **ROC Curve**: To construct the ROC curve, you vary the classification threshold of the logistic regression model and calculate the true positive rate and false positive rate at each threshold. This results in a curve that plots sensitivity against 1 - specificity.\n",
    "\n",
    "4. **AUC-ROC**: The Area Under the ROC Curve (AUC-ROC) is a single value that summarizes the overall performance of the model. The AUC-ROC ranges from 0 to 1, where 0.5 represents a random classifier, and 1 represents a perfect classifier. A higher AUC-ROC indicates better discrimination between the two classes.\n",
    "\n",
    "**Interpreting the ROC Curve**:\n",
    "\n",
    "- A perfect classifier would have an ROC curve that starts at the origin (0,0), goes up to the top-left corner (1,1), and then follows the left and top borders of the plot (forming a right angle).\n",
    "\n",
    "- The closer the ROC curve is to the top-left corner, the better the model's performance, indicating a higher true positive rate and a lower false positive rate across various classification thresholds.\n",
    "\n",
    "- A random classifier would produce an ROC curve close to the diagonal line, with an AUC-ROC of approximately 0.5.\n",
    "\n",
    "- A model with an AUC-ROC less than 0.5 indicates that it is performing worse than random.\n",
    "\n",
    "**How to Use the ROC Curve**:\n",
    "\n",
    "- The ROC curve allows you to visually assess the model's discrimination power across different thresholds. You can choose a threshold that balances the trade-off between true positives and false positives based on your specific problem requirements.\n",
    "\n",
    "- The AUC-ROC value provides a single metric to compare different models. Higher AUC-ROC values are preferred.\n",
    "\n",
    "- By comparing the ROC curves of different models, you can determine which model performs better in terms of classification accuracy and discrimination.\n",
    "\n",
    "In summary, the ROC curve is a valuable tool for evaluating the performance of logistic regression models and other binary classifiers. It helps assess the trade-off between true positive and false positive rates and provides a summarized performance metric (AUC-ROC) that facilitates model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0fc6b-0812-47f6-96ae-8bff470542c2",
   "metadata": {},
   "source": [
    "### Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854f0f0-1109-42b2-91ae-de46252a3b55",
   "metadata": {},
   "source": [
    "Feature selection in logistic regression is the process of choosing a subset of relevant features (independent variables) while discarding irrelevant or redundant ones. Feature selection techniques aim to improve the model's performance by reducing complexity, addressing multicollinearity, and enhancing generalization. Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "1. **Filter Methods**:\n",
    "   - **Correlation**: Calculate the correlation between each feature and the target variable. Select features with the highest correlation values.\n",
    "   - **Chi-Square Test**: Assess the independence between categorical features and the target variable. Select features with significant p-values.\n",
    "\n",
    "   Filter methods help eliminate features that are unrelated to the target variable and are computationally efficient.\n",
    "\n",
    "2. **Wrapper Methods**:\n",
    "   - **Forward Selection**: Start with an empty set of features and iteratively add the most predictive feature based on a chosen criterion (e.g., accuracy). Continue until a predefined stopping criterion is met.\n",
    "   - **Backward Elimination**: Start with all features and iteratively remove the least predictive feature based on a chosen criterion.\n",
    "   - **Recursive Feature Elimination (RFE)**: Similar to backward elimination but involves ranking and eliminating features based on their importance scores from the model (e.g., coefficients).\n",
    "\n",
    "   Wrapper methods assess feature importance within the context of the model's performance, potentially leading to a better feature subset.\n",
    "\n",
    "3. **Embedded Methods**:\n",
    "   - **L1 Regularization (Lasso)**: L1 regularization encourages some coefficients to become exactly zero, effectively performing feature selection. It can automatically eliminate irrelevant features.\n",
    "   - **Tree-Based Feature Selection**: Decision tree-based models, such as Random Forest, can rank features by their importance in predicting the target variable.\n",
    "\n",
    "   Embedded methods are integrated with the model training process, making them efficient and capable of capturing feature dependencies.\n",
    "\n",
    "4. **Information Gain or Mutual Information**: Calculate the information gain or mutual information between each feature and the target variable. These measures help identify features with the highest predictive power.\n",
    "\n",
    "   Information gain and mutual information methods are particularly useful when dealing with both categorical and continuous features.\n",
    "\n",
    "5. **Variance Thresholding**: Eliminate features with low variance. Features with low variance indicate that they do not provide much information as they remain almost constant across the dataset.\n",
    "\n",
    "Feature selection techniques improve model performance by:\n",
    "\n",
    "- Reducing Model Complexity: Fewer features result in simpler models that are less prone to overfitting.\n",
    "- Enhancing Model Interpretability: A smaller set of features makes it easier to understand and explain the model's decision-making process.\n",
    "- Mitigating the Curse of Dimensionality: A reduced feature space can lead to faster model training and inference.\n",
    "- Reducing the Impact of Noisy or Irrelevant Features: Irrelevant features can introduce noise into the model's predictions. Feature selection helps exclude such features.\n",
    "\n",
    "The choice of feature selection technique depends on the nature of the data, the problem, and the specific logistic regression variant used (e.g., binary or multiclass classification). Careful evaluation and validation of feature selection methods are crucial to ensure that the selected subset of features leads to improved model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5f700-a8f8-4c08-8550-40edff6950cd",
   "metadata": {},
   "source": [
    "### Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a1f44-abad-4983-9597-f0cacf7eff09",
   "metadata": {},
   "source": [
    "Handling imbalanced datasets in logistic regression is a common challenge in machine learning, especially in binary classification problems where one class significantly outnumbers the other. The class imbalance issue can lead to biased model performance, where the model is better at predicting the majority class but struggles with the minority class. Several strategies can be used to address class imbalance in logistic regression:\n",
    "\n",
    "1. **Resampling Methods**:\n",
    "   - **Oversampling**: Increase the number of instances in the minority class by duplicating existing instances or generating synthetic samples (e.g., using SMOTE - Synthetic Minority Over-sampling Technique).\n",
    "   - **Undersampling**: Reduce the number of instances in the majority class by randomly removing instances. Undersampling aims to balance class distribution but may result in loss of information.\n",
    "\n",
    "2. **Cost-Sensitive Learning**:\n",
    "   - Adjust the misclassification cost associated with each class. Assign a higher cost to misclassifying instances of the minority class to encourage the model to focus on the minority class.\n",
    "\n",
    "3. **Data Augmentation**:\n",
    "   - Augment the minority class by creating new data points with slight variations from existing minority class instances. This can help increase the diversity of the minority class data.\n",
    "\n",
    "4. **Change the Decision Threshold**:\n",
    "   - By default, logistic regression uses a threshold of 0.5 to classify instances. Adjusting the threshold can lead to a trade-off between precision and recall. Lowering the threshold can increase recall (correctly classifying more minority instances) at the cost of reduced precision, and vice versa.\n",
    "\n",
    "5. **Ensemble Methods**:\n",
    "   - Utilize ensemble techniques like Random Forest or Gradient Boosting, which can handle class imbalance naturally by combining multiple models and giving more importance to misclassified instances.\n",
    "\n",
    "6. **Anomaly Detection**:\n",
    "   - Treat the minority class as an anomaly detection problem and use techniques like One-Class SVM or Isolation Forest to identify instances that do not conform to the majority class distribution.\n",
    "\n",
    "7. **Weighted Logistic Regression**:\n",
    "   - Assign different weights to the classes in the logistic regression model. This is often supported in logistic regression implementations and can give higher importance to the minority class.\n",
    "\n",
    "8. **Collect More Data**:\n",
    "   - If feasible, collect more data for the minority class to balance the dataset. This is often the most effective long-term solution.\n",
    "\n",
    "9. **Evaluate with Appropriate Metrics**:\n",
    "   - When evaluating the model's performance, use metrics such as precision, recall, F1-score, and the area under the ROC curve (AUC-ROC) instead of accuracy. These metrics provide a more comprehensive assessment of how well the model is handling the class imbalance.\n",
    "\n",
    "10. **Hybrid Approaches**:\n",
    "    - Combine multiple strategies. For example, you can oversample the minority class and simultaneously adjust the decision threshold.\n",
    "\n",
    "The choice of strategy depends on the specific dataset, problem, and available resources. It's important to carefully evaluate the impact of each approach and choose the one that best suits the problem and the model's goals. In practice, a combination of these strategies may be the most effective way to address class imbalance in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053674e-c746-4c56-87e5-c67d59345907",
   "metadata": {},
   "source": [
    "### Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da01b32-1fd2-457d-b5ec-449819cd10e4",
   "metadata": {},
   "source": [
    "Implementing logistic regression, like any machine learning technique, can come with various challenges. Here are some common issues that may arise when implementing logistic regression and strategies to address them:\n",
    "\n",
    "1. **Multicollinearity**:\n",
    "   - **Issue**: Multicollinearity occurs when two or more independent variables in the model are highly correlated, making it difficult to discern their individual effects on the dependent variable. This can lead to unstable coefficient estimates.\n",
    "   - **Solution**: \n",
    "     - Identify and remove one of the correlated variables to reduce multicollinearity.\n",
    "     - Use regularization techniques like Ridge or Lasso regression, which penalize the absolute values of coefficients, helping to mitigate multicollinearity.\n",
    "     - Perform dimensionality reduction techniques such as Principal Component Analysis (PCA) to transform the correlated variables into uncorrelated principal components.\n",
    "\n",
    "2. **Overfitting**:\n",
    "   - **Issue**: Overfitting occurs when the model learns to fit the training data noise, resulting in poor generalization to new, unseen data.\n",
    "   - **Solution**:\n",
    "     - Regularize the model by using techniques like Ridge or Lasso regression.\n",
    "     - Collect more data to reduce the risk of overfitting.\n",
    "     - Properly split data into training and test sets for cross-validation.\n",
    "     - Use simpler feature selection methods to reduce model complexity.\n",
    "\n",
    "3. **Imbalanced Datasets**:\n",
    "   - **Issue**: Imbalanced datasets can cause the model to be biased toward the majority class, resulting in poor performance on the minority class.\n",
    "   - **Solution**:\n",
    "     - Apply resampling methods like oversampling, undersampling, or Synthetic Minority Over-sampling Technique (SMOTE).\n",
    "     - Adjust class weights in the logistic regression model.\n",
    "     - Use ensemble techniques like Random Forest or Gradient Boosting that handle class imbalance naturally.\n",
    "\n",
    "4. **Categorical Variables**:\n",
    "   - **Issue**: Logistic regression requires numerical inputs, so categorical variables need to be converted into a suitable numerical format.\n",
    "   - **Solution**:\n",
    "     - Use one-hot encoding to convert categorical variables into binary columns.\n",
    "     - Implement ordinal encoding when there's an ordinal relationship between categories.\n",
    "     - For high-cardinality categorical variables, consider feature engineering or dimensionality reduction.\n",
    "\n",
    "5. **Outliers**:\n",
    "   - **Issue**: Outliers can influence the logistic regression model, leading to biased coefficient estimates.\n",
    "   - **Solution**:\n",
    "     - Identify and handle outliers by transforming or capping extreme values.\n",
    "     - Use robust logistic regression, which is less sensitive to outliers.\n",
    "\n",
    "6. **Model Interpretability**:\n",
    "   - **Issue**: Logistic regression models are relatively interpretable, but complex interactions and nonlinear relationships might be challenging to interpret.\n",
    "   - **Solution**:\n",
    "     - Use feature engineering to create interaction terms that explicitly capture interactions between variables.\n",
    "     - Visualize the results with partial dependence plots or other interpretability tools.\n",
    "     - Use simpler models when interpretability is a primary concern.\n",
    "\n",
    "7. **Data Quality**:\n",
    "   - **Issue**: Poor data quality, such as missing values, can hinder model performance.\n",
    "   - **Solution**:\n",
    "     - Handle missing values through imputation techniques, like mean imputation, median imputation, or more advanced methods.\n",
    "     - Ensure data preprocessing is thorough and that you understand the data's characteristics.\n",
    "\n",
    "8. **Sample Size**:\n",
    "   - **Issue**: Logistic regression requires a sufficient sample size to yield reliable estimates.\n",
    "   - **Solution**:\n",
    "     - Collect more data if possible.\n",
    "     - Use bootstrapping techniques to generate multiple samples from your existing data.\n",
    "\n",
    "9. **Model Evaluation**:\n",
    "   - **Issue**: Selecting the right evaluation metrics and properly validating the model are essential.\n",
    "   - **Solution**:\n",
    "     - Choose appropriate metrics, such as precision, recall, F1-score, and area under the ROC curve (AUC-ROC), depending on the problem.\n",
    "     - Perform cross-validation to assess model performance more accurately.\n",
    "\n",
    "Addressing these challenges and issues is crucial for the successful implementation of logistic regression. The specific strategies chosen will depend on the nature of the problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc84f84-2242-4b52-ab81-2318135aa6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
