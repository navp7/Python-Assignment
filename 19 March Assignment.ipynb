{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d126ddb7-75bf-4e95-b170-b0ed595afdb4",
   "metadata": {},
   "source": [
    "## 19th March Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f62e6-96e4-46fb-bfc7-7cf69e0b814c",
   "metadata": {},
   "source": [
    "## Feature Engineering-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764a42d-5cb7-4a45-a5d6-0f0b666121dd",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f0e54-b419-434f-905f-3eabc3b5f522",
   "metadata": {},
   "source": [
    "**Min-Max scaling**, also known as normalization, is a data preprocessing technique used to scale numeric features to a specific range, usually between 0 and 1. This transformation is achieved by subtracting the minimum value from each data point and then dividing by the range (the difference between the maximum and minimum values). The purpose of Min-Max scaling is to ensure that all features have the same scale, which can be beneficial for certain machine learning algorithms that are sensitive to the scale of features.\n",
    "\n",
    "Mathematically, the formula for Min-Max scaling is:\n",
    "\n",
    "\\[{Scaled Value} ={{Original Value} - {Minimum Value}}/{{Maximum Value} - {Minimum Value}}\n",
    "\\]\n",
    "\n",
    "where the scaled value falls between 0 and 1.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset containing two features: \"Age\" and \"Income.\" The \"Age\" feature ranges from 20 to 70, and the \"Income\" feature ranges from $20,000  to $100,000. If you want to apply Min-Max scaling to these features, the steps are as follows:\n",
    "\n",
    "1. For the \"Age\" feature:\n",
    "   - Minimum value: 20\n",
    "   - Maximum value: 70\n",
    "   - Range: 70 - 20 = 50\n",
    "   - Scaled Age = (Original Age - Minimum Age) / Range\n",
    "   - For an original age of 40: Scaled Age = (40 - 20) / 50 = 0.4\n",
    "\n",
    "2. For the \"Income\" feature:\n",
    "   - Minimum value: $20,000\n",
    "   - Maximum value: $100,000\n",
    "   - Range: $100,000 - $20,000 = $80,000\n",
    "   - Scaled Income = (Original Income - Minimum Income) / Range\n",
    "   - For an original income of $60,000: Scaled Income = ($60,000 - $20,000) / $80,000 = 0.5\n",
    "\n",
    "After applying Min-Max scaling, both the \"Age\" and \"Income\" features will be in the range of 0 to 1, making them comparable in terms of scale.\n",
    "It's important to note that while Min-Max scaling can be effective for some algorithms, it might not be suitable for others that assume a certain distribution of data. Additionally, Min-Max scaling can be influenced by outliers, so it's recommended to handle outliers before applying the scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e011e4c-2d7f-477f-a749-be1b8d684bd8",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00226f58-ba32-48bd-a86c-eebdc7562d20",
   "metadata": {},
   "source": [
    "The **Unit Vector** technique, also known as **Normalization**, is another feature scaling method used in data preprocessing. Unlike Min-Max scaling, which scales features to a specific range, Unit Vector scaling focuses on transforming each data point to have a Euclidean norm (magnitude) of 1 while preserving its direction. This technique is particularly useful when you want to ensure that all data points lie on the same scale without affecting their relative relationships.\n",
    "\n",
    "Mathematically, for each data point {x} with \\n features, the Unit Vector scaling is performed as follows:\n",
    "\n",
    "{Normalized Value} = {x}/|{x}|\n",
    "\n",
    "\n",
    "where |{x}| is the Euclidean norm of the data point.\n",
    "\n",
    "The key difference between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling focuses on the direction of the data points rather than scaling them to a specific range.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with two features: \"Height\" (in inches) and \"Weight\" (in pounds). Let's say you want to apply Unit Vector scaling to these features:\n",
    "\n",
    "1. For the \"Height\" feature:\n",
    "   - Original height: 68 inches\n",
    "   - Euclidean norm: \\(\\sqrt{68^2} = 68\\)\n",
    "   - Normalized Height = Original Height / Euclidean Norm = 68 / 68 = 1\n",
    "\n",
    "2. For the \"Weight\" feature:\n",
    "   - Original weight: 150 pounds\n",
    "   - Euclidean norm: \\(\\sqrt{150^2} = 150\\)\n",
    "   - Normalized Weight = Original Weight / Euclidean Norm = 150 / 150 = 1\n",
    "\n",
    "After applying Unit Vector scaling, both \"Height\" and \"Weight\" features will have a Euclidean norm of 1, ensuring that their magnitudes are comparable while preserving their original directions.\n",
    "\n",
    "Differences between Unit Vector Scaling and Min-Max Scaling:\n",
    "\n",
    "1. **Normalization vs. Range Transformation**:\n",
    "   - Unit Vector scaling focuses on normalizing the data points' directions, making all data points lie on the same scale.\n",
    "   - Min-Max scaling transforms features to a specific range (usually 0 to 1), preserving their magnitudes but potentially changing their relative distances.\n",
    "\n",
    "2. **Euclidean Norm vs. Range Calculation**:\n",
    "   - Unit Vector scaling calculates the Euclidean norm for each data point and scales by that norm.\n",
    "   - Min-Max scaling calculates the range (difference between max and min) for each feature and scales by that range.\n",
    "\n",
    "3. **Use Cases**:\n",
    "   - Unit Vector scaling is suitable when you want to ensure that all data points have equal magnitudes, which is helpful for algorithms sensitive to magnitude like Nearest Neighbors.\n",
    "   - Min-Max scaling is useful when you want to scale features to a specific range, often for algorithms like Support Vector Machines or K-Means clustering.\n",
    "\n",
    "Both scaling techniques have their own advantages and use cases, so the choice between them depends on the characteristics of your data and the requirements of the machine learning algorithm you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985b1f8-82b7-4c3e-b7db-887b7a3dbe83",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51b508-1ef1-4333-8e7b-242d4a9f22cf",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much of the original variance as possible. It achieves this by identifying the principal components, which are linear combinations of the original features that capture the most significant patterns and variability in the data.\n",
    "\n",
    "The basic idea behind PCA is to find a new set of orthogonal axes (principal components) in the data space such that the data's variance is maximized along the first principal component, followed by the second, third, and so on. By projecting the data onto a reduced number of principal components, you can achieve dimensionality reduction while retaining the most important information.\n",
    "\n",
    "Steps of PCA:\n",
    "\n",
    "1. **Standardize the Data**:\n",
    "   Standardize the original data by subtracting the mean and dividing by the standard deviation. This ensures that features are on a similar scale.\n",
    "\n",
    "2. **Compute the Covariance Matrix**:\n",
    "   Calculate the covariance matrix of the standardized data to understand the relationships between features.\n",
    "\n",
    "3. **Compute Eigenvalues and Eigenvectors**:\n",
    "   Compute the eigenvalues and corresponding eigenvectors of the covariance matrix. Eigenvectors represent the directions of maximum variance (principal components).\n",
    "\n",
    "4. **Select Principal Components**:\n",
    "   Sort the eigenvectors based on their corresponding eigenvalues in descending order. Choose the top \\(k\\) eigenvectors to form the new feature space, where \\(k\\) is the desired reduced dimensionality.\n",
    "\n",
    "5. **Project Data onto Principal Components**:\n",
    "   Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset with two features: \"Height\" and \"Weight\" of individuals. You want to reduce the dimensionality using PCA to capture the main patterns in the data.\n",
    "\n",
    "1. Standardize the data by subtracting the mean and dividing by the standard deviation for both \"Height\" and \"Weight.\"\n",
    "\n",
    "2. Compute the covariance matrix of the standardized data.\n",
    "\n",
    "3. Calculate the eigenvalues and eigenvectors of the covariance matrix. Let's say you get two eigenvectors: \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\).\n",
    "\n",
    "4. Since you want to reduce the dimensionality to one component, choose the eigenvector with the highest eigenvalue, which is \\(\\mathbf{v}_1\\).\n",
    "\n",
    "5. Project the original data onto \\(\\mathbf{v}_1\\) to obtain the reduced-dimensional representation.\n",
    "\n",
    "The resulting lower-dimensional representation captures the main patterns in the data along the direction of maximum variance.\n",
    "\n",
    "PCA is widely used in various fields, including image processing, data compression, and feature extraction, as it allows you to reduce the dimensionality of data while retaining important information for analysis or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405129d7-e95a-4c27-9364-0807050d19e7",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb45f3-0f8b-488a-b566-e913e8cbc81b",
   "metadata": {},
   "source": [
    "**PCA (Principal Component Analysis)** and **Feature Extraction** are closely related concepts in machine learning and data analysis. PCA can be used as a technique for feature extraction, which involves transforming the original features into a new set of features (principal components) that capture the most important information in the data.\n",
    "\n",
    "The relationship between PCA and Feature Extraction:\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   Both PCA and feature extraction aim to reduce the dimensionality of the data by representing it in a lower-dimensional space. However, while PCA focuses on capturing as much variance as possible, feature extraction aims to extract the most relevant information for a specific task.\n",
    "\n",
    "2. **Information Compression**:\n",
    "   Both techniques can be seen as methods to compress the information present in the original features while maintaining the most significant aspects.\n",
    "\n",
    "Using PCA for Feature Extraction:\n",
    "\n",
    "Let's consider an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose you have a dataset containing images of handwritten digits (e.g., digits 0-9) for digit recognition. Each image is represented as a vector of pixel values. Each pixel can be considered as a feature, and since images are typically high-dimensional, you want to reduce the dimensionality while preserving the distinctive characteristics of each digit.\n",
    "\n",
    "1. **Dataset Preparation**:\n",
    "   You have a dataset of grayscale images, each represented as a vector of pixel values.\n",
    "\n",
    "2. **Standardize Data**:\n",
    "   Standardize the pixel values across all images (center the data).\n",
    "\n",
    "3. **Compute Covariance Matrix**:\n",
    "   Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "4. **Eigenvalues and Eigenvectors**:\n",
    "   Compute the eigenvalues and corresponding eigenvectors of the covariance matrix.\n",
    "\n",
    "5. **Select Principal Components**:\n",
    "   Sort the eigenvectors based on their eigenvalues and select the top \\(k\\) eigenvectors that capture most of the variance. These eigenvectors represent the directions of maximum variability in the images.\n",
    "\n",
    "6. **Project Data onto Principal Components**:\n",
    "   Project the original image data onto the selected principal components to obtain a lower-dimensional representation. These new features, the principal components, are a linear combination of the original pixel values.\n",
    "\n",
    "By using the top \\(k\\) principal components, you've effectively performed feature extraction. The new features capture the essential patterns and variations in the images, which are crucial for digit recognition.\n",
    "\n",
    "In this example, PCA serves as a feature extraction technique that reduces the dimensionality of the pixel space while retaining the critical characteristics of the images. The resulting reduced-dimensional representation can be used as input for a classification algorithm to recognize handwritten digits more efficiently than using the original high-dimensional pixel values.\n",
    "\n",
    "Overall, PCA for feature extraction is a powerful tool that helps improve the performance of machine learning algorithms, especially when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2cbc08-174e-4ffe-8411-014539cf6a95",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb1a9e-f3ad-427c-aec9-65e389436e5c",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the dataset and ensure that the features are on a similar scale. This is important because recommendation algorithms often involve calculating distances or similarities between features, and having features on different scales can lead to biased recommendations. Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1. **Understand the Features**:\n",
    "   Examine the features in your dataset, such as \"price,\" \"rating,\" and \"delivery time.\" Check their ranges and distributions to understand how they vary.\n",
    "\n",
    "2. **Standardization vs. Min-Max Scaling**:\n",
    "   While standardization (z-score normalization) is another popular scaling technique, Min-Max scaling is appropriate when you want to transform features to a specific range, typically between 0 and 1. Since features like \"price\" and \"rating\" have natural lower and upper bounds (minimum price, maximum rating), Min-Max scaling can be suitable in this case.\n",
    "\n",
    "3. **Calculate Minimum and Maximum Values**:\n",
    "   For each feature, calculate the minimum and maximum values present in the dataset. This can be done using simple statistics or built-in functions in libraries like NumPy.\n",
    "\n",
    "4. **Apply Min-Max Scaling Formula**:\n",
    "   For each data point in the dataset and for each feature, apply the Min-Max scaling formula:\n",
    "   \n",
    "   \\[\n",
    "   \\text{Scaled Value} = \\frac{\\text{Original Value} - \\text{Minimum Value}}{\\text{Maximum Value} - \\text{Minimum Value}}\n",
    "   \\]\n",
    "\n",
    "   This formula scales the original feature values to the range [0, 1], where 0 corresponds to the minimum value, and 1 corresponds to the maximum value.\n",
    "\n",
    "5. **Updated Dataset**:\n",
    "   Replace the original feature values with the scaled values in the dataset. The resulting dataset will have features transformed to the [0, 1] range.\n",
    "\n",
    "For instance, let's say you have a dataset containing the following sample data:\n",
    "\n",
    "| Price ($) | Rating (out of 5) | Delivery Time (minutes) |\n",
    "|-----------|-------------------|-----------------------|\n",
    "| 12        | 4.8               | 30                    |\n",
    "| 20        | 4.2               | 45                    |\n",
    "| 8         | 3.5               | 25                    |\n",
    "\n",
    "You would perform Min-Max scaling as follows:\n",
    "\n",
    "1. Calculate the minimum and maximum values for each feature:\n",
    "   - Minimum Price: 8\n",
    "   - Maximum Price: 20\n",
    "   - Minimum Rating: 3.5\n",
    "   - Maximum Rating: 4.8\n",
    "   - Minimum Delivery Time: 25\n",
    "   - Maximum Delivery Time: 45\n",
    "\n",
    "2. Apply Min-Max scaling formula to each feature's values:\n",
    "\n",
    "   - Scaled Price = \\(({Original Price} - 8) / (20 - 8)\\)\n",
    "   - Scaled Rating = \\(({Original Rating} - 3.5) / (4.8 - 3.5)\\)\n",
    "   - Scaled Delivery Time = \\(({Original Delivery Time} - 25) / (45 - 25)\\)\n",
    "\n",
    "The resulting dataset will have the scaled values for each feature, ensuring that they are within the [0, 1] range. This scaled dataset is now suitable for use in recommendation algorithms that rely on similarity or distance calculations between features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572140e5-1afa-48db-b456-61fe29c76f72",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a0d8f-58ee-4f83-8da1-ff3ba7bf45c4",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices using a dataset with numerous features, **Principal Component Analysis (PCA)** can be a valuable technique to reduce the dimensionality of the data while preserving as much variance as possible. Reducing dimensionality is essential to mitigate the \"curse of dimensionality\" and enhance the model's performance by simplifying the input space. Here's how you would use PCA to achieve dimensionality reduction for predicting stock prices:\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "   Begin by thoroughly understanding the features in your dataset. These features might include financial data, market trends, sentiment analysis scores, or other relevant information that could influence stock prices.\n",
    "\n",
    "2. **Standardize the Data**:\n",
    "   Before applying PCA, it's important to standardize the features by subtracting the mean and dividing by the standard deviation. Standardization ensures that features with larger scales don't dominate the PCA process.\n",
    "\n",
    "3. **Calculate Covariance Matrix**:\n",
    "   Compute the covariance matrix of the standardized data. The covariance matrix quantifies the relationships between features and forms the basis of PCA.\n",
    "\n",
    "4. **Calculate Eigenvalues and Eigenvectors**:\n",
    "   Compute the eigenvalues and corresponding eigenvectors of the covariance matrix. Eigenvectors represent the directions of maximum variance (principal components), and eigenvalues indicate the amount of variance captured by each eigenvector.\n",
    "\n",
    "5. **Sort Eigenvectors by Eigenvalues**:\n",
    "   Sort the eigenvectors in descending order based on their corresponding eigenvalues. Eigenvectors with larger eigenvalues capture more variance and are more informative.\n",
    "\n",
    "6. **Select Principal Components**:\n",
    "   Choose the top \\(k\\) eigenvectors that collectively capture a substantial portion of the variance in the data. The goal is to retain a high percentage of the total variance (e.g., 95% or more).\n",
    "\n",
    "7. **Project Data onto Principal Components**:\n",
    "   Project the original standardized data onto the selected \\(k\\) principal components to obtain a lower-dimensional representation. This reduces the number of features while retaining the most important information.\n",
    "\n",
    "8. **Interpret the Results**:\n",
    "   Examine the variance explained by each principal component to understand the relative importance of each component. You can use scree plots to visualize the eigenvalues and determine the \"elbow point,\" which indicates the number of principal components to retain.\n",
    "\n",
    "9. **Use Reduced-Dimensional Data for Modeling**:\n",
    "   The transformed data using the selected principal components can now be used as input features for your stock price prediction model. Since the dimensionality has been reduced, the model might be more efficient and less prone to overfitting.\n",
    "\n",
    "It's important to note that while PCA reduces dimensionality, it may also lead to a loss of interpretability, as the new features are linear combinations of the original features. Therefore, you should carefully analyze the trade-off between dimensionality reduction and interpretability in the context of your stock price prediction project. Additionally, PCA might not always be the best approach if there are specific domain insights that indicate certain features are crucial for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff63f0-80d4-4d6e-a9bc-c97500f19872",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09735da8-0192-4cf0-a9e3-33e4b68d065a",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling and transform the values in the dataset [1, 5, 10, 15, 20] to a range of -1 to 1, you can follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb991c97-d055-4535-9d46-1687fa4eb338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Values: [ 1  5 10 15 20]\n",
      "Scaled Values: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "values = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate minimum and maximum values\n",
    "min_value = values.min()\n",
    "max_value = values.max()\n",
    "\n",
    "# Apply Min-Max scaling formula\n",
    "scaled_values = ((values - min_value) / (max_value - min_value)) * 2 - 1\n",
    "\n",
    "print(\"Original Values:\", values)\n",
    "print(\"Scaled Values:\", scaled_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2412b5-ab53-499f-a45a-f3e59961a5e9",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bad55-3b6f-449f-bcb7-1b96d36ba7ec",
   "metadata": {},
   "source": [
    "When performing feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], the choice of how many principal components to retain depends on the amount of variance you want to preserve in the data. The goal is to retain a sufficient amount of variance while reducing the dimensionality of the data. Here's how to approach this:\n",
    "\n",
    "1. **Standardize the Data**:\n",
    "   Start by standardizing the features to have mean 0 and standard deviation 1. This ensures that all features are on a similar scale and avoids features with larger scales dominating the PCA.\n",
    "\n",
    "2. **Calculate Covariance Matrix and Eigenvalues**:\n",
    "   Compute the covariance matrix of the standardized data and then calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "3. **Sort Eigenvectors by Eigenvalues**:\n",
    "   Sort the eigenvectors in descending order based on their corresponding eigenvalues. Eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. **Choose Number of Principal Components**:\n",
    "   Decide how many principal components to retain based on the cumulative explained variance. A common approach is to choose the number of principal components that collectively capture a significant percentage of the total variance. You can plot a \"scree plot\" to visualize the eigenvalues and their cumulative explained variance.\n",
    "\n",
    "5. **Set Explained Variance Threshold**:\n",
    "   Determine a threshold for the cumulative explained variance that you consider satisfactory. For example, you might aim to retain 95% or 99% of the total variance.\n",
    "\n",
    "6. **Select Principal Components**:\n",
    "   Choose the minimum number of principal components required to reach or exceed the desired threshold of explained variance.\n",
    "\n",
    "For instance, if you find that the first two principal components explain a total of 90% of the variance, and your threshold is 95%, you might choose to retain these two principal components.\n",
    "\n",
    "The choice of how many principal components to retain depends on the trade-off between dimensionality reduction and the amount of variance explained. Retaining fewer principal components results in a lower-dimensional representation of the data but may lead to a loss of information. On the other hand, retaining more principal components maintains more information but might not provide as significant dimensionality reduction.\n",
    "\n",
    "In the context of your specific dataset with features [height, weight, age, gender, blood pressure], you would follow these steps to calculate the cumulative explained variance and then determine the appropriate number of principal components to retain. The choice of how many principal components to keep ultimately depends on your specific goals and the amount of variance you consider important to capture in the reduced-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a5627-40a4-4b81-aa39-93b9d7fff3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
